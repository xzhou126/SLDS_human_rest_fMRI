{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import resting_state_summaries as rss\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.image import load_img, new_img_like\n",
    "import nilearn.plotting as plotting\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize by each run\n",
    "def normalize_timeseries(datas):\n",
    "    for i in range(len(datas)):\n",
    "        ts = datas[i]\n",
    "        ts = (ts - np.expand_dims(ts.mean(axis=0), axis=0))/np.expand_dims(ts.std(axis=0), axis=0)\n",
    "        datas[i] = ts\n",
    "    return(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzhou126/miniconda3/envs/slds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "K = 6\n",
    "D = 10\n",
    "with open('Final_model/K6_D10_500subjs_compact_model.pkl', 'rb') as f:\n",
    "    [model, q, elbos, q_z] = pickle.load(f)\n",
    "\n",
    "num_roi = model.N\n",
    "num_subject = len(np.unique(model.tags))\n",
    "pid = np.unique(model.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/roi_timeseries_rsfMRI_HCP_held_out', 'rb') as f:\n",
    "    datas = pickle.load(f)\n",
    "with open('data/tags_rsfMRI_HCP_held_out', 'rb') as f:\n",
    "    tags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_at_late_time(y_bundles, late_times):\n",
    "    K = len(y_bundles)\n",
    "    num_roi = y_bundles[0][0].shape[1]\n",
    "    activity_at_late_time = np.zeros((K, num_roi))\n",
    "    for k in range(K):  \n",
    "        for j in range(num_roi):\n",
    "            t = late_times[k]\n",
    "                # collect timeseries values from bundles that durates at least (t+1) time steps\n",
    "            bold_t = []\n",
    "            for i in range(len(y_bundles[k])):\n",
    "                if len(y_bundles[k][i])>=(t+1):\n",
    "                    bold_t.append(y_bundles[k][i][:,j][t])  # activity of jth ROI at time t of state entry\n",
    "            # calculate mean responses\n",
    "            activity_at_late_time[k,j] = np.mean(bold_t)\n",
    "    return activity_at_late_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = model.parent.emissions.Cs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summary_data/state_duration.pkl','rb') as f:\n",
    "   state_duration = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group-level: state importance evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_subject = dict()\n",
    "y_subject = dict()\n",
    "z_bundle_subject = dict()\n",
    "A_roi_subject = dict()\n",
    "degree_subject = dict()\n",
    "for s in range(num_subject):\n",
    "    z_subject[s] = [q_z[i] for i in range(len(datas)) if tags[i] == pid[s]]\n",
    "    y_subject[s] = [datas[i] for i in range(len(datas)) if tags[i] == pid[s]]\n",
    "    z_bundle_subject[s] = rss.collect_z_bundle(z_subject[s])\n",
    "    num_run = 4\n",
    "    # use subject-level dynamics matrices\n",
    "    A_roi = []\n",
    "    for k in range(K):\n",
    "        A_roi.append(C.dot(model.children[pid[s]].dynamics.As[k]).dot(C.T))\n",
    "    \n",
    "    degree = np.zeros((K, num_roi))\n",
    "    for k in range(K):\n",
    "        for j in range(num_roi):\n",
    "            degree[k,j] = np.linalg.norm(A_roi[k][:,j])\n",
    "\n",
    "    A_roi_subject[s] = A_roi\n",
    "    degree_subject[s] = degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_subject = dict()\n",
    "y_subject = dict()\n",
    "z_bundle_subject = dict()\n",
    "importance_timeseries_subject = dict()\n",
    "importance_bundles_subject = dict()\n",
    "for s in range(num_subject):\n",
    "    # 1. importance timeseries for the subject\n",
    "    z_subject[s] = [q_z[i] for i in range(len(datas)) if tags[i] == pid[s]]\n",
    "    y_subject[s] = [datas[i] for i in range(len(datas)) if tags[i] == pid[s]]\n",
    "    z_bundle_subject[s] = rss.collect_z_bundle(z_subject[s])\n",
    "    num_run = 4\n",
    "    importance_timeseries_subject[s] = []\n",
    "    # use subject-level dynamics matrices\n",
    "    A_roi = []\n",
    "    for k in range(K):\n",
    "        A_roi.append(C.dot(model.children[pid[s]].dynamics.As[k]).dot(C.T))\n",
    "    degree = np.zeros((K, num_roi))\n",
    "    for k in range(K):\n",
    "        for j in range(num_roi):\n",
    "            degree[k,j] = np.linalg.norm(A_roi[k][:,j])\n",
    "    for rid in range(num_run):\n",
    "        data_in_run = y_subject[s][rid]\n",
    "        states_in_run = z_subject[s][rid]\n",
    "        y_importance = np.multiply(np.abs(data_in_run), degree[states_in_run,:])\n",
    "        importance_timeseries_subject[s].append(y_importance)\n",
    "    # normalize\n",
    "    importance_timeseries_subject[s] = normalize_timeseries(importance_timeseries_subject[s])\n",
    "    \n",
    "    # 2. compute subject-level importance evolution, analogous to activity evolution  \n",
    "    importance_bundles_subject[s] = rss.collect_y_bundle(np.concatenate(importance_timeseries_subject[s]), z_bundle_subject[s], K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('summary_data/importance_timeseries.pkl','wb') as f:\n",
    "#     pickle.dump(importance_timeseries, f)\n",
    "# with open('summary_data/importance_timeseries_subject.pkl','wb') as f:\n",
    "#     pickle.dump(importance_timeseries_subject, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine importance map at group level\n",
    "importance_bundles = dict()\n",
    "for k in range(K):\n",
    "    importance_bundles[k] = []\n",
    "    for s in range(num_subject):\n",
    "        for j in range(len(importance_bundles_subject[s][k])):\n",
    "            importance_bundles[k].append(importance_bundles_subject[s][k][j])\n",
    "importance_map = activity_at_late_time(importance_bundles, state_duration + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the null of equal importance (permutation test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_map_subject = dict()\n",
    "for s in range(num_subject):\n",
    "    importance_map_subject[s] = activity_at_late_time(importance_bundles_subject[s], state_duration + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1000\n",
    "z_score = np.zeros((num_subject, K, num_roi))\n",
    "for s in range(num_subject):\n",
    "    null_maps = []\n",
    "    for b in range(B):\n",
    "        spatial_shuffle = np.arange(num_roi)\n",
    "        np.random.shuffle(spatial_shuffle) \n",
    "        null_maps.append(importance_map_subject[s][:, spatial_shuffle])\n",
    "    mu = np.mean(null_maps, axis = 0)\n",
    "    sigma = np.std(null_maps, axis = 0, ddof=1)\n",
    "    z_score[s,:,:] = (importance_map_subject[s] - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_mat = np.zeros((K,num_roi))\n",
    "mu_0 = 0\n",
    "for k in range(K):\n",
    "    for i in range(num_roi):\n",
    "                scores = z_score[:,k,i]\n",
    "                N = num_subject-np.sum(np.isnan(scores))\n",
    "                test_statistics = np.sqrt(N)*(np.nanmean(scores)-mu_0)/np.nanstd(scores)\n",
    "                df = N-1\n",
    "                pvalue_mat[k,i] = scipy.stats.t.sf(test_statistics, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues_corrected = scipy.stats.false_discovery_control(pvalue_mat)\n",
    "rejection_mat = pvalues_corrected<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('summary_data/region_importance_rejection_mat.pkl','wb') as f:\n",
    "#     pickle.dump(rejection_mat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summary_data/state_order.pkl','rb') as f:\n",
    "   state_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importance map, cortex (thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = load_img('Schaefer2018_200Parcels_17Networks_order_FSLMNI152_2mm.nii.gz')\n",
    "def vec_to_img(vec):\n",
    "    n_rois = 200\n",
    "    atlas_data = atlas.get_fdata()\n",
    "\n",
    "    vec_img_data = np.zeros_like(atlas_data)\n",
    "    for idx_roi in range(n_rois):\n",
    "        vec_img_data += (atlas_data==idx_roi+1) * vec[idx_roi]\n",
    "\n",
    "    vec_img = new_img_like(data=vec_img_data, ref_niimg=atlas)\n",
    "    return vec_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageChops\n",
    "def trim(im):\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    #Bounding box given as a 4-tuple defining the left, upper, right, and lower pixel coordinates.\n",
    "    #If the image is completely empty, this method returns None.\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        return im.crop(bbox)\n",
    "    \n",
    "def join_2x2(imgs, out_path, dpi=300, padding=20):\n",
    "    if len(imgs) != 4:\n",
    "        raise ValueError(\"Need exactly 4 images in order: TL, TR, BL, BR\")\n",
    "\n",
    "    w1, h1 = imgs[0].size\n",
    "    w2, h2 = imgs[1].size\n",
    "    w3, h3 = imgs[2].size\n",
    "    w4, h4 = imgs[3].size\n",
    "\n",
    "    top_row_h = max(h1, h2)\n",
    "    bottom_row_h = max(h3, h4)\n",
    "    total_w = max(w1+w2,w3+w4) + padding\n",
    "    total_h = top_row_h + bottom_row_h + padding\n",
    "\n",
    "    grid = Image.new(\"RGBA\", (total_w, total_h), (0, 0, 0, 0))\n",
    "    grid.paste(imgs[0], (0, 0))\n",
    "    grid.paste(imgs[1], (w1+padding, 0))\n",
    "    grid.paste(imgs[2], (0, top_row_h+padding))\n",
    "    grid.paste(imgs[3], (w3+padding, top_row_h+padding))\n",
    "\n",
    "    grid.save(out_path, dpi=(dpi, dpi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholded maps\n",
    "for k in range(K):\n",
    "    state = state_order[k]\n",
    "    vec = importance_map[state,:].copy()\n",
    "    vec[rejection_mat[state, :] == False] = 0 \n",
    "    vec_img = vec_to_img(vec[54:254])\n",
    "    grid = []\n",
    "    for view in ['lateral', 'medial']:\n",
    "        for hemi in ['left', 'right']:\n",
    "            fig,ax = plotting.plot_img_on_surf(\n",
    "                        vec_img,\n",
    "                        surf_mesh='fsaverage5', bg_on_data=True, inflate=True,\n",
    "                        hemispheres=[hemi], views=[view],\n",
    "                        vmin = -.3, vmax=0.3, \n",
    "                        threshold=1e-10,\n",
    "                        colorbar=False,\n",
    "                        cmap = 'seismic'\n",
    "                    )\n",
    "            fig.set_size_inches(4, 4)\n",
    "            fig.savefig(\"maps/region_importance/importance_map_thresholded/state%s_%s_%s.png\"%(k+1, hemi, view), dpi=300, transparent=True)\n",
    "            plt.close(fig)\n",
    "            grid.append(trim(Image.open(\"maps/region_importance/importance_map_thresholded/state%s_%s_%s.png\"%(k+1, hemi, view))))\n",
    "    join_2x2(grid, \"maps/region_importance/importance_map_thresholded/state%s.png\"%(k+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importance map, subcortex (thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 11,  0,  2,  0,  0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.sum(rejection_mat[state_order, 0:54], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = load_img('Schaefer2018_200Parcels_7Networks_order_Tian_Subcortex_S4_MNI152NLin6Asym_2mm.nii.gz')\n",
    "def vec_to_img(vec):\n",
    "    n_rois = 254\n",
    "    atlas_data = atlas.get_fdata()\n",
    "\n",
    "    vec_img_data = np.zeros_like(atlas_data)\n",
    "    for idx_roi in range(n_rois):\n",
    "        vec_img_data += (atlas_data==idx_roi+1) * vec[idx_roi]\n",
    "\n",
    "    vec_img = new_img_like(data=vec_img_data, ref_niimg=atlas)\n",
    "    return vec_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    state = state_order[k]\n",
    "    vec = importance_map[state,:].copy()\n",
    "    vec[rejection_mat[state,:] == False]  = 0\n",
    "    vec[54:254] = 0     # set entire cortex to zero, display subcortex only\n",
    "    vec_img = vec_to_img(vec)\n",
    "    vec_img.to_filename(f'maps/region_importance/importance_map_subcortex_thresholded/state%s.nii.gz'%(k+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
